# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aqBcZA7C4OZpl8zXao1C0X-RA9Uf_G-s
"""

# --- Cell 1: Imports and Setup ---
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import LabelEncoder
import pickle
import os
import warnings

# Ignore specific warnings that might clutter the output (optional)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)


print("--- Setup ---")
# Define the path to your already uploaded CSV file
invoices_file_path = '/content/invoices.csv'
model_save_path = "/content/" # Save directly in /content/
model_filename = "anomaly_model.pkl"
encoder_filename = "encoders.pkl"
model_filepath = os.path.join(model_save_path, model_filename)
encoder_filepath = os.path.join(model_save_path, encoder_filename)


# Check if the invoice file exists
if not os.path.exists(invoices_file_path):
    raise FileNotFoundError(f"Error: The file '{invoices_file_path}' was not found in the Colab environment. Please make sure it's uploaded to /content/")
else:
    print(f"Using invoice data file: {invoices_file_path}")
    print(f"Model will be saved to/loaded from: {model_save_path}")

# --- Cell 2: Data Loading and Preprocessing Definition ---
print("\n--- Defining Preprocessing Function ---")
# (preprocess_data function remains the same as the corrected previous version)
def preprocess_data(csv_file_path):
    """Loads, preprocesses, and returns the invoice data, label encoders, and columns used."""
    print(f"Reading CSV from: {csv_file_path}")
    try:
        # Read CSV, try to infer initial types but be ready to correct
        df = pd.read_csv(csv_file_path, low_memory=False)
        print(f"Initial DataFrame shape: {df.shape}")
    except FileNotFoundError:
        print(f"Error: CSV file not found at {csv_file_path}")
        return None, None, None
    except Exception as e:
        print(f"Error reading CSV: {e}")
        return None, None, None

    le_dict = {}  # Store label encoders
    categorical_cols = ['first_name', 'last_name', 'email', 'address', 'city', 'stock_code', 'job']
    numeric_cols_base = ['qty', 'amount', 'product_id'] # Base numeric columns

    # Create a copy for processing
    df_processed = df.copy()

    # 1. Handle Categorical Columns
    print("Encoding categorical columns...")
    for col in categorical_cols:
        if col in df_processed.columns: # Check if column exists
            le = LabelEncoder()
            # Convert to string robustly, handling potential NaN/mixed types
            df_processed[col] = df_processed[col].fillna('missing').astype(str)
            df_processed[col] = le.fit_transform(df_processed[col])
            le_dict[col] = le
        else:
            print(f"Warning: Categorical column '{col}' not found. Skipping.")

    # 2. Handle Date Column
    print("Processing invoice date...")
    if 'invoice_date' in df_processed.columns:
        df_processed['invoice_date'] = pd.to_datetime(df_processed['invoice_date'], format='%d/%m/%Y', errors='coerce')
        original_rows = len(df_processed)
        df_processed.dropna(subset=['invoice_date'], inplace=True)
        rows_dropped = original_rows - len(df_processed)
        if rows_dropped > 0:
            print(f"Warning: Dropped {rows_dropped} rows due to invalid date formats in 'invoice_date'.")

        if not df_processed.empty:
            df_processed['invoice_day'] = df_processed['invoice_date'].dt.day
            df_processed['invoice_month'] = df_processed['invoice_date'].dt.month
            df_processed['invoice_year'] = df_processed['invoice_date'].dt.year
            df_processed.drop('invoice_date', axis=1, inplace=True)
            date_cols_created = ['invoice_day', 'invoice_month', 'invoice_year']
        else:
             print("Warning: DataFrame became empty after handling invalid dates. No data to process.")
             return None, le_dict, None
    else:
        print("Warning: 'invoice_date' column not found. Date features not created.")
        date_cols_created = []

    # 3. Identify Final Columns for Model Training
    final_model_cols = [col for col in numeric_cols_base if col in df_processed.columns]
    final_model_cols.extend(le_dict.keys())
    final_model_cols.extend(date_cols_created)
    final_model_cols = sorted(list(dict.fromkeys(final_model_cols)))
    final_model_cols = [col for col in final_model_cols if col in df_processed.columns]

    if not final_model_cols:
        print("Error: No valid columns identified for model training after preprocessing.")
        return None, le_dict, None

    print(f"Columns selected for model training: {final_model_cols}")
    df_final = df_processed[final_model_cols].copy()

    # 4. Final Conversion to Numeric and Handling NaNs
    for col in df_final.columns:
         df_final[col] = pd.to_numeric(df_final[col], errors='coerce')

    initial_nan_count = df_final.isnull().sum().sum()
    if initial_nan_count > 0:
        print(f"Warning: Found {initial_nan_count} NaN values after processing. Filling with 0.")
        df_final.fillna(0, inplace=True)

    print("Final DataFrame info before returning:")
    df_final.info(verbose=False) # Less verbose output

    return df_final, le_dict, df_final.columns


# --- Cell 3: Model Training and Saving Definition ---
print("\n--- Defining Model Training Function ---")
# (train_and_save_model function remains the same as the corrected previous version)
def train_and_save_model(df, le_dict, model_filename="anomaly_model.pkl", encoder_filename="encoders.pkl", save_path="/content/"):
    """Trains the Isolation Forest model and saves it along with the encoders."""
    if df is None or df.empty:
        print("Error: DataFrame is empty or None. Cannot train model.")
        return None, None

    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns
    if not non_numeric_cols.empty:
        print(f"ERROR: Non-numeric columns detected before fitting: {non_numeric_cols.tolist()}. Check preprocessing.")
        for col in non_numeric_cols:
             df[col] = pd.to_numeric(df[col], errors='coerce')
        df.fillna(0, inplace=True)
        non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns
        if not non_numeric_cols.empty:
            print("ERROR: Could not convert all columns to numeric. Aborting training.")
            return None, None
        else:
             print("Warning: Had to perform extra numeric conversion before fitting.")


    if df.isnull().values.any():
        print("Warning: NaN values detected before fitting. Filling with 0.")
        df.fillna(0, inplace=True)

    model = IsolationForest(contamination=0.01, random_state=42)
    print(f"Fitting Isolation Forest model on {df.shape[0]} samples and {df.shape[1]} features...")

    try:
        model.fit(df)
        print("Model fitting complete.")
    except Exception as e:
        print(f"Error fitting the model: {e}")
        print("Columns:", df.columns)
        print("Data types:\n", df.dtypes)
        print("Sample data:\n", df.head())
        return None, None

    model_filepath = os.path.join(save_path, model_filename)
    encoder_filepath = os.path.join(save_path, encoder_filename)

    try:
        with open(model_filepath, 'wb') as f:
            pickle.dump(model, f)
        with open(encoder_filepath, 'wb') as f:
            pickle.dump(le_dict, f)
        print(f"Model successfully saved to: {model_filepath}")
        print(f"Encoders successfully saved to: {encoder_filepath}")
        return model_filepath, encoder_filepath
    except Exception as e:
        print(f"Error saving model or encoders to {save_path}: {e}")
        return None, None

# --- Cell 4: Anomaly Detection Definition ---
print("\n--- Defining Anomaly Detection Function ---")
def detect_anomaly(model_filepath="/content/anomaly_model.pkl",
                   encoder_filepath="/content/encoders.pkl",
                   df_columns_trained=None, # Pass the column names from the training df
                   original_data_path=None): # Path to original CSV for single-feature model fitting
    """Loads the saved model/encoders and performs anomaly detection based on user input."""

    if not os.path.exists(model_filepath) or not os.path.exists(encoder_filepath):
        print(f"Error: Model ('{model_filepath}') or encoder file ('{encoder_filepath}') not found. Please train and save first.")
        return None

    if df_columns_trained is None:
        print("Error: DataFrame column names from training are required.")
        return None

    if original_data_path is None:
        print("Error: Path to original data CSV is required for single-feature checks.")
        return None

    try:
        with open(model_filepath, 'rb') as f:
            model = pickle.load(f)
        with open(encoder_filepath, 'rb') as f:
            le_dict = pickle.load(f)
        print("Model and encoders loaded successfully.")
    except Exception as e:
        print(f"Error loading model or encoders: {e}")
        return None

    categorical_cols = list(le_dict.keys())
    all_possible_cols = df_columns_trained.tolist()
    date_cols_created = ['invoice_day', 'invoice_month', 'invoice_year']

    user_input_cols = [col for col in all_possible_cols if col not in date_cols_created]
    if any(col in all_possible_cols for col in date_cols_created):
        user_input_cols.append('invoice_date')
    user_input_cols = sorted(list(set(user_input_cols)))

    # --- Get User Input ---
    while True:
        prompt_text = f"\nEnter column to check (options: {', '.join(user_input_cols)}, all, quit): "
        anomaly_col_input = input(prompt_text).strip().lower()

        if anomaly_col_input == 'quit':
            return "Exiting anomaly detection."
        elif anomaly_col_input == 'all':
            input_data = {}
            print("\nEnter values for the fields you want to check (press Enter to skip):")
            for col in user_input_cols:
                 if col == 'invoice_date' or col in df_columns_trained:
                     val_str = input(f"Enter value for '{col}': ").strip()
                     if val_str:
                         input_data[col] = val_str
            if not input_data:
                print("No input provided for 'all'. Exiting.")
                return None
            break
        elif anomaly_col_input in user_input_cols:
            val_str = input(f"Enter the value for '{anomaly_col_input}': ").strip()
            if not val_str:
                 print(f"No value entered for {anomaly_col_input}. Please try again.")
                 continue
            input_data = {anomaly_col_input: val_str}
            break
        else:
            print(f"Invalid column name. Please choose from the options or type 'all'/'quit'.")

    # --- Prepare Input DataFrame (df_input) ---
    df_input = pd.DataFrame(0.0, index=[0], columns=df_columns_trained)
    processed_input_flag = False
    original_input_values = {}

    for col_input, val_str in input_data.items():
        original_input_values[col_input] = val_str
        processed_this_iter = False
        try:
            target_col = col_input # The actual column name in the dataframe
            if col_input == 'invoice_date':
                date_val = pd.to_datetime(val_str, format='%Y-%m-%d')
                if 'invoice_day' in df_input.columns: df_input.loc[0,'invoice_day'] = float(date_val.day)
                if 'invoice_month' in df_input.columns: df_input.loc[0,'invoice_month'] = float(date_val.month)
                if 'invoice_year' in df_input.columns: df_input.loc[0,'invoice_year'] = float(date_val.year)
                processed_this_iter = True
            elif col_input in le_dict:
                le = le_dict[col_input]
                if val_str in le.classes_:
                     df_input.loc[0, target_col] = float(le.transform([val_str])[0])
                else:
                     print(f"Warning: Unseen label '{val_str}' for column '{target_col}'. Treating as 0. Anomaly likely.")
                     df_input.loc[0, target_col] = 0.0
                processed_this_iter = True
            elif col_input in df_columns_trained: # Numeric columns
                 df_input.loc[0, target_col] = float(val_str)
                 processed_this_iter = True

            if processed_this_iter:
                processed_input_flag = True

        except ValueError:
             print(f"Invalid value format '{val_str}' for column '{col_input}'. Please enter a valid number or date (YYYY-MM-DD). Using 0.")
             if target_col in df_input.columns: df_input.loc[0, target_col] = 0.0
        except Exception as e:
             print(f"Error processing input for {col_input} ('{val_str}'): {e}. Using 0.")
             if target_col in df_input.columns: df_input.loc[0, target_col] = 0.0


    if not processed_input_flag :
        print("Could not process any of the provided input values meaningfully.")
        return None

    df_input.fillna(0.0, inplace=True)

    # --- Make Prediction ---
    anomaly_messages = []
    anomaly_detected = False

    try:
        df_input_final = df_input[df_columns_trained] # Ensure columns match training

        if anomaly_col_input == 'all':
            print("Predicting based on the combination of provided inputs...")
            prediction = model.predict(df_input_final)
            if prediction[0] == -1:
                 anomaly_detected = True
                 processed_vals_str = ', '.join([f"{k}: {original_input_values.get(k, 'N/A')}" for k in input_data])
                 anomaly_messages.append(f"Anomaly detected based on the combination of inputs: {processed_vals_str}")

        else: # Single feature check
            print(f"Predicting anomaly independently for '{anomaly_col_input}'...")
            temp_df, _, _ = preprocess_data(original_data_path) # Reload original preprocessed data
            if temp_df is None:
                print(f"Error: Could not reload original data from '{original_data_path}' for single feature check.")
                return "Error during single feature check."

            cols_to_fit_predict = []
            if anomaly_col_input == 'invoice_date':
                cols_to_fit_predict = [col for col in date_cols_created if col in df_columns_trained]
            elif anomaly_col_input in df_columns_trained:
                cols_to_fit_predict = [anomaly_col_input]

            if not cols_to_fit_predict:
                 print(f"Warning: Could not find required columns for '{anomaly_col_input}' in training data.")
            else:
                # Ensure columns exist in both temp_df and df_input_final
                cols_to_fit_predict = [col for col in cols_to_fit_predict if col in temp_df.columns and col in df_input_final.columns]
                if not cols_to_fit_predict:
                     print(f"Error: Mismatch in columns for single feature check ('{anomaly_col_input}').")
                else:
                    try:
                        temp_model = IsolationForest(contamination=0.01, random_state=42) # Consistent parameters
                        temp_model.fit(temp_df[cols_to_fit_predict]) # Fit only on the specific column(s)
                        prediction = temp_model.predict(df_input_final[cols_to_fit_predict])
                        if prediction[0] == -1:
                            anomaly_detected = True
                            anomaly_messages.append(f"Anomaly detected in {anomaly_col_input}: {original_input_values.get(anomaly_col_input, 'N/A')}")
                    except Exception as fit_e:
                        print(f"Error fitting/predicting temporary model for {anomaly_col_input}: {fit_e}")

    except Exception as e:
        print(f"Error during prediction phase: {e}")
        return "An error occurred during prediction."


    if anomaly_detected:
        return "\n".join(anomaly_messages)
    else:
        # --- Corrected f-string for the 'no anomaly' message ---
        if anomaly_col_input != 'all':
            input_desc = f"input ({anomaly_col_input}: {original_input_values.get(anomaly_col_input, 'N/A')})"
        else:
            combined_inputs = ', '.join([f'{k}: {original_input_values.get(k, "N/A")}' for k in input_data])
            input_desc = f"combination of inputs: {combined_inputs}"
        # Now use the variable 'input_desc' safely
        return f"No anomaly detected for the provided {input_desc}."


# === Jupyter Notebook Workflow (for Colab with file at /content/) ===

# --- Cell 1: Preprocess Data ---
print("--- Preprocessing Data ---")
df_train, le_dict, df_cols_trained = preprocess_data(invoices_file_path)

if df_train is not None:
    print("\nPreprocessing complete.")
else:
    print("\nPreprocessing failed. Cannot proceed.")
    df_cols_trained = None # Ensure it's None if preprocessing fails

# --- Cell 2: Train and Save Model (Run only once unless data changes) ---
model_filepath, encoder_filepath = None, None
if df_train is not None and le_dict is not None:
    print("\n--- Training and Saving Model ---")
    model_filepath, encoder_filepath = train_and_save_model(df_train, le_dict, save_path=model_save_path)
    if model_filepath and encoder_filepath:
        print("\nModel training and saving complete.")
        print("\nFiles in /content/:")
        # Use try-except in case /content doesn't exist locally
        try:
            print(os.listdir('/content/'))
        except FileNotFoundError:
            print("'/content/' directory not found (may not be in Colab).")
    else:
        print("\nModel training or saving failed.")
else:
    print("\nSkipping model training due to preprocessing errors.")


# --- Cell 3: Detect Anomalies (Run interactively as needed) ---
if model_filepath and encoder_filepath and df_cols_trained is not None:
    print("\n--- Anomaly Detection ---")
    while True:
        result = detect_anomaly(model_filepath=model_filepath,
                                encoder_filepath=encoder_filepath,
                                df_columns_trained=df_cols_trained,
                                original_data_path=invoices_file_path)
        if result:
            print("\n--- Result ---")
            print(result)
            if result == "Exiting anomaly detection.":
                break
        else:
            print("\nAnomaly detection could not be performed or no anomalies found.")

        another_check = input("\nCheck another anomaly? (yes/no): ").strip().lower()
        if another_check != 'yes':
            break

    print("\nAnomaly detection finished.")
else:
    print("\nModel not trained or saved correctly. Cannot perform anomaly detection.")